{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEB CRAWLING Identitas : Nurul Vicky Wahdaniah 160411100128 Mata Kuliah Penambangan dan Pencarian Web","title":"Home"},{"location":"#web-crawling","text":"","title":"WEB CRAWLING"},{"location":"#identitas","text":"Nurul Vicky Wahdaniah 160411100128 Mata Kuliah Penambangan dan Pencarian Web","title":"Identitas :"},{"location":"web1/","text":"WEB CRAWLING (Web Content Mining) Web Content Mining merupakan proses untuk mengambil,mengolah, memanipulasi data Text, Video, dan lain - lain pada sebuah website. Pendahuluan Halaman ini berisi tentang langkah - langkah crawling data dari sebuah website, kemudian dilanjutkan dengan pre-processing, seleksi fitur hingga clustering data. Tools and Requirements** Website URL : https://www.malasngoding.com/ Python 3.6 Python Libraries : BeautifulSoup4 Berfungsi untuk mengcrawl data pada website. Cara menginstall BeautifulSoup4 menggunakan pip : pip install beautifulsoup4 SQLite3 Berfungsi sebagai Database untuk penyimpanan data dari website. CSV Berfungsi untuk read and write tabular data dalam format CSV. Cara menginstall CSV module menggunakan pip : pip install python-csv Sastrawi Berfungsi untuk steeming pada bagian pre-processing data. Steeming bertujuan untuk mentransformasikan kata menjadi kata dasar tanpa imbuhan, baik awalan, akhiran atau sisipan. Casa menginstall Sastrawi menggunakan pip : pip install Sastrawi Math Berfungsi pada proses-proses yang menggunakan fungsi matematika. Cara menginstall modul Math menggunakan pip : pip install math Numpy Berfungsi untuk operasi vektor dan matriks untuk keperluan analisis data. Cara menginstall Numpy menggunakan pip : pip install numpy Sckit-Learn Berfungsi memberikan sejumlah fitur untuk keperluan data science seperti Algoritma Regresi, Algoritma Naive Bayes, Algoritma Clustering, Algoritma Decision Tree, Data Prepocessing Tool, dan lain - lain. Cara menginstall Sckit-Learn menggunakan pip : pip install scikit-learn Sckit-Fuzzy Merupakan kumpulan dari algoritma fuzzy yang digunakan pada modul SciPy atau SkLearn. Cara menginstall Sckit-Fuzzy menggunakan pip : pip install scikit-fuzzy Database KBI Digunakan pada proses pre-processing untuk memisahkan kata penting pada data. Crawling Website URL : https://www.malasngoding.com/ Data yang diambil dari website di atas berupa data Text, yaitu Judul Artikel dan Isi Artikel. Langkah Proses Crawling : import library yang akan digunakan untuk crawl data. from bs4 import BeautifulSoup Request pada halaman web tertuju. src = \"https://www.malasngoding.com/\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') simpan elemen/data (judul dan isi) yang akan di crawl pada variabel berdasar class pada tag html web tertuju. linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') code di atas merupakan variabel penampung untuk link yang menuju judul dan isi artikel konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') code di atas merupakan variabel yang menampung judul dan isi artikel Masukkan data ke dalam Database. conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); conn.commit() export data ke dalam format CSV def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Kode Program Lengkap Proses Crawling : conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"https://www.malasngoding.com/\" n = 1 while n <= 2: print(\"page \",n) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); except AttributeError: continue conn.commit() src = pagination['href'] n+=1 def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Pre-Processing Pre-processing merupakan tahapan dimana program melakukan seleksi data yang akan di proses. Proses Pre-Processing meliputi : Case Folding Case Folding dibutuhkan dalam mengkonversi keseluruhan text dalam dokumen menjadi suatu bentuk standar/lower case. Lebih ringkasnya, case folding mengubah semua huruf dalam dokumen menjadi huruf kecil. hara huruf 'a' sampai 'z' yang diterima. Karakter selain huruf akan dihilangkan. Tokenizing Tahap tokenizing merupakan tahap pemecahan kalimat menjadi beberapa kata tunggal. Filtering (Stopword) Tahap filtering adalah tahap mengambil kata-kata penting dari hasil token. Stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Stemming Sremming diperlukan untuk memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapat imbuhan yang berbeda. Kode Program Pre-Processing cursor = conn.execute(\"SELECT* from ARTICLES\") isif ='' for row in cursor: isif+=row[1] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) write_csv(\"kata_before_%s.csv\"%n, katadasar) conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata TF - IDF TF (Term Frequency) TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. IDF (Inverse Document Frequency) IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. Kode Program TF-IDF #TF df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) #IDF idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) write_csv(\"tfidf_%s.csv\"%n, tfidf) Feature Selection Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi. Kode Program Seleksi Fitur def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2) Clustering Clustering merupakan proses pengelompokan data dengan karakteristik yang sama ke suatu kelompok dan data dengan karakteristik berbeda ke kelompok yang lain. Metode yang digunakan dalam clustering ini yaitu menggunakan metode K- Mean dengan pendekatan Fuzzy. Setelah data di cluster, langkah berikutnya adalah menghitung nilai koefisien Silhouette. Kode Program Clustering print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i])) print(s_avg) Kode Keseluruhan Program import requests from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from math import log10 import numpy as np from sklearn.metrics import silhouette_samples, silhouette_score import skfuzzy as fuzz conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"https://www.malasngoding.com/\" n = 1 while n <= 2: print(\"page \",n) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); except AttributeError: continue conn.commit() src = pagination['href'] n+=1 #function write csv def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) #preproccessing - stopword(menghilangkan imbuhan) cursor = conn.execute(\"SELECT* from ARTICLES\") isif ='' for row in cursor: isif+=row[1] ##print(row) factory = StopWordRemoverFactory() #katadasar stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) #steming - mengambil kata dasar stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] #cursor = conn.execute(\"SELECT* from ARTIKEL\") for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) #print(katadasar) write_csv(\"kata_before_%s.csv\"%n, katadasar) #nganu kbi #Sharing kata Sesuai KBI Belum VSM conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata #print(berhasil) #menghitung tf-idf = menghitung jumlah fitur/kata #df = menghitung frekuensi - conn = sqlite3.connect('articles.sqlite') matrix2=[] cursor = conn.execute(\"SELECT* from ARTICLES\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[1].lower().count(i)) #print(tampung) #print(row[2]) matrix2.append(tampung) #print(matrix2) #import csv kata yg sesuai dengan KBI write_csv(\"kata_after_%s.csv\"%n, berhasil) #tf-idf df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) write_csv(\"tfidf_%s.csv\"%n, tfidf) #seleksi fitur def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2) #clustering print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i]))#+\"\\t\" + str(silhouette[i])) print(s_avg) #kmeans = KMeans(n_clusters=3, random_state=0).fit(xBaru) #print(kmeans.labels_) write_csv(\"Cluster%sFS8.csv\"%n, [[\"Cluster\"]]) write_csv(\"Cluster%sFS8.csv\"%n, [membership], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [[\"silhouette\"]], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [silhouette], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [[\"Keanggotaan\"]], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, u, \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [[\"pusat Cluster\"]], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, cntr, \"a\") References https://www.malasngoding.com/ http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://yudiagusta.wordpress.com/clustering/ https://informatikalogi.com/text-preprocessing/amp/ https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://informatikalogi.com/term-weighting-tf-idf/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/","title":"Web Content Mining"},{"location":"web1/#web-crawling-web-content-mining","text":"Web Content Mining merupakan proses untuk mengambil,mengolah, memanipulasi data Text, Video, dan lain - lain pada sebuah website.","title":"WEB CRAWLING (Web Content Mining)"},{"location":"web1/#pendahuluan","text":"Halaman ini berisi tentang langkah - langkah crawling data dari sebuah website, kemudian dilanjutkan dengan pre-processing, seleksi fitur hingga clustering data.","title":"Pendahuluan"},{"location":"web1/#tools-and-requirements","text":"Website URL : https://www.malasngoding.com/ Python 3.6 Python Libraries : BeautifulSoup4 Berfungsi untuk mengcrawl data pada website. Cara menginstall BeautifulSoup4 menggunakan pip : pip install beautifulsoup4 SQLite3 Berfungsi sebagai Database untuk penyimpanan data dari website. CSV Berfungsi untuk read and write tabular data dalam format CSV. Cara menginstall CSV module menggunakan pip : pip install python-csv Sastrawi Berfungsi untuk steeming pada bagian pre-processing data. Steeming bertujuan untuk mentransformasikan kata menjadi kata dasar tanpa imbuhan, baik awalan, akhiran atau sisipan. Casa menginstall Sastrawi menggunakan pip : pip install Sastrawi Math Berfungsi pada proses-proses yang menggunakan fungsi matematika. Cara menginstall modul Math menggunakan pip : pip install math Numpy Berfungsi untuk operasi vektor dan matriks untuk keperluan analisis data. Cara menginstall Numpy menggunakan pip : pip install numpy Sckit-Learn Berfungsi memberikan sejumlah fitur untuk keperluan data science seperti Algoritma Regresi, Algoritma Naive Bayes, Algoritma Clustering, Algoritma Decision Tree, Data Prepocessing Tool, dan lain - lain. Cara menginstall Sckit-Learn menggunakan pip : pip install scikit-learn Sckit-Fuzzy Merupakan kumpulan dari algoritma fuzzy yang digunakan pada modul SciPy atau SkLearn. Cara menginstall Sckit-Fuzzy menggunakan pip : pip install scikit-fuzzy Database KBI Digunakan pada proses pre-processing untuk memisahkan kata penting pada data.","title":"Tools and Requirements**"},{"location":"web1/#crawling","text":"Website URL : https://www.malasngoding.com/ Data yang diambil dari website di atas berupa data Text, yaitu Judul Artikel dan Isi Artikel.","title":"Crawling"},{"location":"web1/#langkah-proses-crawling","text":"","title":"Langkah Proses Crawling :"},{"location":"web1/#import-library-yang-akan-digunakan-untuk-crawl-data","text":"from bs4 import BeautifulSoup","title":"import library yang akan digunakan untuk crawl data."},{"location":"web1/#request-pada-halaman-web-tertuju","text":"src = \"https://www.malasngoding.com/\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser')","title":"Request pada halaman web tertuju."},{"location":"web1/#simpan-elemendata-judul-dan-isi-yang-akan-di-crawl-pada-variabel-berdasar-class-pada-tag-html-web-tertuju","text":"linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') code di atas merupakan variabel penampung untuk link yang menuju judul dan isi artikel konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') code di atas merupakan variabel yang menampung judul dan isi artikel","title":"simpan elemen/data (judul dan isi) yang akan di crawl pada variabel berdasar class pada tag html web tertuju."},{"location":"web1/#masukkan-data-ke-dalam-database","text":"conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); conn.commit()","title":"Masukkan data ke dalam Database."},{"location":"web1/#export-data-ke-dalam-format-csv","text":"def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row)","title":"export data ke dalam format CSV"},{"location":"web1/#kode-program-lengkap-proses-crawling","text":"conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"https://www.malasngoding.com/\" n = 1 while n <= 2: print(\"page \",n) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); except AttributeError: continue conn.commit() src = pagination['href'] n+=1 def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row)","title":"Kode Program Lengkap Proses Crawling :"},{"location":"web1/#pre-processing","text":"Pre-processing merupakan tahapan dimana program melakukan seleksi data yang akan di proses. Proses Pre-Processing meliputi : Case Folding Case Folding dibutuhkan dalam mengkonversi keseluruhan text dalam dokumen menjadi suatu bentuk standar/lower case. Lebih ringkasnya, case folding mengubah semua huruf dalam dokumen menjadi huruf kecil. hara huruf 'a' sampai 'z' yang diterima. Karakter selain huruf akan dihilangkan. Tokenizing Tahap tokenizing merupakan tahap pemecahan kalimat menjadi beberapa kata tunggal. Filtering (Stopword) Tahap filtering adalah tahap mengambil kata-kata penting dari hasil token. Stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Stemming Sremming diperlukan untuk memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapat imbuhan yang berbeda.","title":"Pre-Processing"},{"location":"web1/#kode-program-pre-processing","text":"cursor = conn.execute(\"SELECT* from ARTICLES\") isif ='' for row in cursor: isif+=row[1] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) write_csv(\"kata_before_%s.csv\"%n, katadasar) conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata","title":"Kode Program Pre-Processing"},{"location":"web1/#tf-idf","text":"","title":"TF - IDF"},{"location":"web1/#tf-term-frequency","text":"TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar.","title":"TF (Term Frequency)"},{"location":"web1/#idf-inverse-document-frequency","text":"IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar.","title":"IDF (Inverse Document Frequency)"},{"location":"web1/#kode-program-tf-idf","text":"#TF df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) #IDF idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) write_csv(\"tfidf_%s.csv\"%n, tfidf)","title":"Kode Program TF-IDF"},{"location":"web1/#feature-selection","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi.","title":"Feature Selection"},{"location":"web1/#kode-program-seleksi-fitur","text":"def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2)","title":"Kode Program Seleksi Fitur"},{"location":"web1/#clustering","text":"Clustering merupakan proses pengelompokan data dengan karakteristik yang sama ke suatu kelompok dan data dengan karakteristik berbeda ke kelompok yang lain. Metode yang digunakan dalam clustering ini yaitu menggunakan metode K- Mean dengan pendekatan Fuzzy. Setelah data di cluster, langkah berikutnya adalah menghitung nilai koefisien Silhouette.","title":"Clustering"},{"location":"web1/#kode-program-clustering","text":"print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i])) print(s_avg)","title":"Kode Program Clustering"},{"location":"web1/#kode-keseluruhan-program","text":"import requests from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from math import log10 import numpy as np from sklearn.metrics import silhouette_samples, silhouette_score import skfuzzy as fuzz conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"https://www.malasngoding.com/\" n = 1 while n <= 2: print(\"page \",n) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); except AttributeError: continue conn.commit() src = pagination['href'] n+=1 #function write csv def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) #preproccessing - stopword(menghilangkan imbuhan) cursor = conn.execute(\"SELECT* from ARTICLES\") isif ='' for row in cursor: isif+=row[1] ##print(row) factory = StopWordRemoverFactory() #katadasar stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) #steming - mengambil kata dasar stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] #cursor = conn.execute(\"SELECT* from ARTIKEL\") for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) #print(katadasar) write_csv(\"kata_before_%s.csv\"%n, katadasar) #nganu kbi #Sharing kata Sesuai KBI Belum VSM conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata #print(berhasil) #menghitung tf-idf = menghitung jumlah fitur/kata #df = menghitung frekuensi - conn = sqlite3.connect('articles.sqlite') matrix2=[] cursor = conn.execute(\"SELECT* from ARTICLES\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[1].lower().count(i)) #print(tampung) #print(row[2]) matrix2.append(tampung) #print(matrix2) #import csv kata yg sesuai dengan KBI write_csv(\"kata_after_%s.csv\"%n, berhasil) #tf-idf df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) write_csv(\"tfidf_%s.csv\"%n, tfidf) #seleksi fitur def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2) #clustering print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i]))#+\"\\t\" + str(silhouette[i])) print(s_avg) #kmeans = KMeans(n_clusters=3, random_state=0).fit(xBaru) #print(kmeans.labels_) write_csv(\"Cluster%sFS8.csv\"%n, [[\"Cluster\"]]) write_csv(\"Cluster%sFS8.csv\"%n, [membership], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [[\"silhouette\"]], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [silhouette], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [[\"Keanggotaan\"]], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, u, \"a\") write_csv(\"Cluster%sFS8.csv\"%n, [[\"pusat Cluster\"]], \"a\") write_csv(\"Cluster%sFS8.csv\"%n, cntr, \"a\")","title":"Kode Keseluruhan Program"},{"location":"web1/#references","text":"https://www.malasngoding.com/ http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://yudiagusta.wordpress.com/clustering/ https://informatikalogi.com/text-preprocessing/amp/ https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://informatikalogi.com/term-weighting-tf-idf/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/","title":"References"},{"location":"web2/","text":"WEB CRAWLING (Web Structure Mining) Web Structure Mining juga dikenal sebagai \"link mining\" merupakan sebuah proses untuk mengetahui hubungan antara sebuah link pada web dengan direct linknya. Pendahuluan Halaman ini berisi tentang langkah - langkah crawling link dari sebuah website yaitu https://wiraraja.ac.id/ , kemudian dilanjutkan dengan penggambaran Graph yang menunjukkan alur link - link dalam website tersebut. Hasil akhir dari pengerjaan ini adalah menampilkan graph berarah yang menunjukkan jalur link dan juga menampilkan seluruh link yang terkait dan pagerank masing-masing. Link dan Pagerank ditampilkan urut berdasar nilai PageRank yang terkecil. Screen Capture Hasil Running Program : Graph : https://drive.google.com/open?id=1MKDNJNsAWy-RMmNfAor1fVPRe53yvJHd DataFrame Pagerank : https://drive.google.com/open?id=1A5voK4bZsPYIvwzsze7j98M-Fp7kNcEA https://drive.google.com/open?id=1CzPCsnvv-CCUZUVo4vNddYty2oOIc9ck Tools and Requirements** Website URL : https://wiraraja.ac.id/ Python 3.6 Python Libraries : BeautifulSoup4 Berfungsi untuk mengcrawl data pada website. Cara menginstall BeautifulSoup4 menggunakan pip : pip install beautifulsoup4 Networkx NetworkX adalah library Python untuk pembuatan, manipulasi, dan studi tentang struktur, dinamika, dan fungsi jaringan yang kompleks. pip install networkx Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. Terdapat plot untuk menampilkan data secara 2D atau 3D. pip install matplotlib Pandas Pandas berfungsi untuk memuat file ke dalam tabel Virtual yang disimpan pada RAM. Pandas dapat mengolah suatu data dan mengolahnya seperti join , distinct , group by , agregasi, dan teknik seperti pada SQL. pip install pandas Proses Website URL : https://wiraraja.ac.id/ Data yang diambil dari website di atas adalah Link yang terdapat pada website hingga kedalaman 3. Langkah Proses Crawling : import library yang akan digunakan untuk crawl link. import requests from bs4 import BeautifulSoup import pandas as pd import networkx as nx import matplotlib.pyplot as plt Membuat class fungsi sebelumnya buat kelas fungsi yang akan digunakan untuk mempermudah pengerjaan Class simplifiedURL() class fungsi ini akan digunakan untuk mengoreksi/pengecekan link / url website awal yang akan kita crawl dengan 3 kali pengecekan yaitu pada Protokol, Subdomain dan tanda \"/\" pada akhir url. def simplifiedURL(url): if \"www.\" in url: ind = url.index(\"www.\") url = url[ind:] if not \"http\" in url: url = \"http://\"+url if url[-1] == \"/\": url = url[:-1] parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url Class crawl() class fungsi ini digunakan untuk mencrawl seluruh link menggunakan parameter url dan kedalaman, kemudian menampilkan seluruh link yang diambil. def crawl(url, max_deep, show=False, deep=0, done=[]): global edgelist url = simplifiedURL(url) deep += 1 if not url in done: links = getLink(url) done.append(url) if show: if deep == 1: print(\"(%d)%s\" %(len(links),url)) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep) Meng-crawl link menggunakan BeautifulSoup Class getLink() berfungsi untuk mengambil seluruh link pada website dan memasukkan ke dalam list yang mengembalikan nilai list. def getLink(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') a = soup.findAll('a') temp = [] for i in a : try: link = i['href'] if not link in temp and 'http' in link : temp.append(link) except KeyError: pass return temp #print(temp) except: return list() root = \"https://wiraraja.ac.id/\" edgelist = [] s = True crawl(root, 3, show=s) //memanggil fungsi crawl() setelah link diambil menggunakan class getLink(), selanjutnya yaitu memanggil fungsi crawl() yang telah dibuat sebelumnya untuk mengambil link yang terletak di kedalaman selanjutnya, dan kemudian memasukkan mereka kedalam list \"edgelist\". Membuat DataFrame untuk menjadikan list menjadi tabel. edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\")) Membuat Graph dari link dalam DataFrame g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.random_layout(g) \"nx.random_layout(g)\" digunakan untuk menentukan tipe graph dengan tipe random. Menghitung Pagerank pr = nx.pagerank(g) Menampilkan Pagerank dan Link nodelist = [root] nodelist = g.nodes label= {} data = [] inisialisasi variabel for i, key in enumerate(nodelist): data.append((pr[key],key)) label[key]=i Kode diatas berfungsi untuk melakukan perulangan untuk menambahkan/menggabungkan link dengan pageranknya ke dalam list \"data\" pd.set_option('display.max_rows', 100) Kode diatas digunakan untuk menampilkan semua baris pada data frame. Biasanya penggunaan data frame tidak akan menampilkan seluruh kolom/baris didalamnya, melainkan hari beberapa kolom/baris saja. tabel = pd.DataFrame(data, columns=(\"Pagerank\", \"Links\")) Kode diatas berfungsi untuk membuat data frame dengan 2 kolom dengan value pagerank dan link pada list \"data\" sebelumnya. u = tabel.sort_values(by=[\"Pagerank\", \"Links\"], ascending=[True,False]) Kode diatas digukanan untuk mengurutkan tabel berdasarkan nilai pagerank dari nilai terkecil hingga terbesar. print(u) Menampilkan DataFrame/Tabel Menggambar dan Menampilkan Graph nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label) Kode diatas berfungsi untuk menggambar Graph plt.axis(\"off\") plt.show() Kode diatas untuk menampilkan Graph pada Window baru. Kode Program Lengkap Proses : import requests from bs4 import BeautifulSoup import pandas as pd import networkx as nx import matplotlib.pyplot as plt def simplifiedURL(url): if \"www.\" in url: ind = url.index(\"www.\") url = url[ind:] if not \"http\" in url: url = \"http://\"+url if url[-1] == \"/\": url = url[:-1] parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url def crawl(url, max_deep, show=False, deep=0, done=[]): global edgelist url = simplifiedURL(url) deep += 1 if not url in done: links = getLink(url) done.append(url) if show: if deep == 1: print(\"(%d)%s\" %(len(links),url)) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep) def getLink(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') a = soup.findAll('a') temp = [] for i in a : try: link = i['href'] if not link in temp and 'http' in link : temp.append(link) except KeyError: pass return temp #print(temp) except: return list() root = \"https://wiraraja.ac.id/\" s = True edgelist = [] crawl(root, 3, show=s) edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\")) #print(edgelist) g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.random_layout(g) pr = nx.pagerank(g) print(\"-----\") nodelist = [root] nodelist = g.nodes label= {} data = [] for i, key in enumerate(nodelist): data.append((pr[key],key)) label[key]=i pd.set_option('display.max_rows', 100) tabel = pd.DataFrame(data, columns=(\"Pagerank\", \"Links\")) u = tabel.sort_values(by=[\"Pagerank\", \"Links\"], ascending=[True,False]) print(u) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label) plt.axis(\"off\") plt.show() References https://wiraraja.ac.id/ < https://codeburst.io/web-crawling-and-scraping-in-python-7116b16d27c7 > https://stackoverflow.com/questions/17618981/how-to-sort-pandas-data-frame-using-values-from-several-columns https://networkx.github.io/ https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97 https://www.geeksforgeeks.org/python-pandas-dataframe/ https://stackoverflow.com/questions/50018054/how-to-print-all-rows-of-a-pandas-dataframe-in-the-pycharm-console?noredirect=1&lq=1 https://www.youtube.com/watch?v=uIcime2nBjs https://www.tutorialride.com/data-mining/web-mining.htm https://dzone.com/refcardz/data-mining-discovering-and?chapter=8","title":"Web Structure Mining"},{"location":"web2/#web-crawling-web-structure-mining","text":"Web Structure Mining juga dikenal sebagai \"link mining\" merupakan sebuah proses untuk mengetahui hubungan antara sebuah link pada web dengan direct linknya.","title":"WEB CRAWLING (Web Structure Mining)"},{"location":"web2/#pendahuluan","text":"Halaman ini berisi tentang langkah - langkah crawling link dari sebuah website yaitu https://wiraraja.ac.id/ , kemudian dilanjutkan dengan penggambaran Graph yang menunjukkan alur link - link dalam website tersebut. Hasil akhir dari pengerjaan ini adalah menampilkan graph berarah yang menunjukkan jalur link dan juga menampilkan seluruh link yang terkait dan pagerank masing-masing. Link dan Pagerank ditampilkan urut berdasar nilai PageRank yang terkecil. Screen Capture Hasil Running Program : Graph : https://drive.google.com/open?id=1MKDNJNsAWy-RMmNfAor1fVPRe53yvJHd DataFrame Pagerank : https://drive.google.com/open?id=1A5voK4bZsPYIvwzsze7j98M-Fp7kNcEA https://drive.google.com/open?id=1CzPCsnvv-CCUZUVo4vNddYty2oOIc9ck","title":"Pendahuluan"},{"location":"web2/#tools-and-requirements","text":"Website URL : https://wiraraja.ac.id/ Python 3.6 Python Libraries : BeautifulSoup4 Berfungsi untuk mengcrawl data pada website. Cara menginstall BeautifulSoup4 menggunakan pip : pip install beautifulsoup4 Networkx NetworkX adalah library Python untuk pembuatan, manipulasi, dan studi tentang struktur, dinamika, dan fungsi jaringan yang kompleks. pip install networkx Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. Terdapat plot untuk menampilkan data secara 2D atau 3D. pip install matplotlib Pandas Pandas berfungsi untuk memuat file ke dalam tabel Virtual yang disimpan pada RAM. Pandas dapat mengolah suatu data dan mengolahnya seperti join , distinct , group by , agregasi, dan teknik seperti pada SQL. pip install pandas","title":"Tools and Requirements**"},{"location":"web2/#proses","text":"Website URL : https://wiraraja.ac.id/ Data yang diambil dari website di atas adalah Link yang terdapat pada website hingga kedalaman 3.","title":"Proses"},{"location":"web2/#langkah-proses-crawling","text":"","title":"Langkah Proses Crawling :"},{"location":"web2/#import-library-yang-akan-digunakan-untuk-crawl-link","text":"import requests from bs4 import BeautifulSoup import pandas as pd import networkx as nx import matplotlib.pyplot as plt","title":"import library yang akan digunakan untuk crawl link."},{"location":"web2/#membuat-class-fungsi","text":"sebelumnya buat kelas fungsi yang akan digunakan untuk mempermudah pengerjaan Class simplifiedURL() class fungsi ini akan digunakan untuk mengoreksi/pengecekan link / url website awal yang akan kita crawl dengan 3 kali pengecekan yaitu pada Protokol, Subdomain dan tanda \"/\" pada akhir url. def simplifiedURL(url): if \"www.\" in url: ind = url.index(\"www.\") url = url[ind:] if not \"http\" in url: url = \"http://\"+url if url[-1] == \"/\": url = url[:-1] parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url Class crawl() class fungsi ini digunakan untuk mencrawl seluruh link menggunakan parameter url dan kedalaman, kemudian menampilkan seluruh link yang diambil. def crawl(url, max_deep, show=False, deep=0, done=[]): global edgelist url = simplifiedURL(url) deep += 1 if not url in done: links = getLink(url) done.append(url) if show: if deep == 1: print(\"(%d)%s\" %(len(links),url)) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep)","title":"Membuat class fungsi"},{"location":"web2/#meng-crawl-link-menggunakan-beautifulsoup","text":"Class getLink() berfungsi untuk mengambil seluruh link pada website dan memasukkan ke dalam list yang mengembalikan nilai list. def getLink(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') a = soup.findAll('a') temp = [] for i in a : try: link = i['href'] if not link in temp and 'http' in link : temp.append(link) except KeyError: pass return temp #print(temp) except: return list() root = \"https://wiraraja.ac.id/\" edgelist = [] s = True crawl(root, 3, show=s) //memanggil fungsi crawl() setelah link diambil menggunakan class getLink(), selanjutnya yaitu memanggil fungsi crawl() yang telah dibuat sebelumnya untuk mengambil link yang terletak di kedalaman selanjutnya, dan kemudian memasukkan mereka kedalam list \"edgelist\".","title":"Meng-crawl link menggunakan BeautifulSoup"},{"location":"web2/#membuat-dataframe-untuk-menjadikan-list-menjadi-tabel","text":"edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\"))","title":"Membuat DataFrame untuk menjadikan list menjadi tabel."},{"location":"web2/#membuat-graph-dari-link-dalam-dataframe","text":"g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.random_layout(g) \"nx.random_layout(g)\" digunakan untuk menentukan tipe graph dengan tipe random.","title":"Membuat Graph dari link dalam DataFrame"},{"location":"web2/#menghitung-pagerank","text":"pr = nx.pagerank(g)","title":"Menghitung Pagerank"},{"location":"web2/#menampilkan-pagerank-dan-link","text":"nodelist = [root] nodelist = g.nodes label= {} data = [] inisialisasi variabel for i, key in enumerate(nodelist): data.append((pr[key],key)) label[key]=i Kode diatas berfungsi untuk melakukan perulangan untuk menambahkan/menggabungkan link dengan pageranknya ke dalam list \"data\" pd.set_option('display.max_rows', 100) Kode diatas digunakan untuk menampilkan semua baris pada data frame. Biasanya penggunaan data frame tidak akan menampilkan seluruh kolom/baris didalamnya, melainkan hari beberapa kolom/baris saja. tabel = pd.DataFrame(data, columns=(\"Pagerank\", \"Links\")) Kode diatas berfungsi untuk membuat data frame dengan 2 kolom dengan value pagerank dan link pada list \"data\" sebelumnya. u = tabel.sort_values(by=[\"Pagerank\", \"Links\"], ascending=[True,False]) Kode diatas digukanan untuk mengurutkan tabel berdasarkan nilai pagerank dari nilai terkecil hingga terbesar. print(u) Menampilkan DataFrame/Tabel","title":"Menampilkan Pagerank dan Link"},{"location":"web2/#menggambar-dan-menampilkan-graph","text":"nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label) Kode diatas berfungsi untuk menggambar Graph plt.axis(\"off\") plt.show() Kode diatas untuk menampilkan Graph pada Window baru.","title":"Menggambar dan Menampilkan Graph"},{"location":"web2/#kode-program-lengkap-proses","text":"import requests from bs4 import BeautifulSoup import pandas as pd import networkx as nx import matplotlib.pyplot as plt def simplifiedURL(url): if \"www.\" in url: ind = url.index(\"www.\") url = url[ind:] if not \"http\" in url: url = \"http://\"+url if url[-1] == \"/\": url = url[:-1] parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url def crawl(url, max_deep, show=False, deep=0, done=[]): global edgelist url = simplifiedURL(url) deep += 1 if not url in done: links = getLink(url) done.append(url) if show: if deep == 1: print(\"(%d)%s\" %(len(links),url)) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep) def getLink(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') a = soup.findAll('a') temp = [] for i in a : try: link = i['href'] if not link in temp and 'http' in link : temp.append(link) except KeyError: pass return temp #print(temp) except: return list() root = \"https://wiraraja.ac.id/\" s = True edgelist = [] crawl(root, 3, show=s) edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\")) #print(edgelist) g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.random_layout(g) pr = nx.pagerank(g) print(\"-----\") nodelist = [root] nodelist = g.nodes label= {} data = [] for i, key in enumerate(nodelist): data.append((pr[key],key)) label[key]=i pd.set_option('display.max_rows', 100) tabel = pd.DataFrame(data, columns=(\"Pagerank\", \"Links\")) u = tabel.sort_values(by=[\"Pagerank\", \"Links\"], ascending=[True,False]) print(u) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label) plt.axis(\"off\") plt.show()","title":"Kode Program Lengkap Proses  :"},{"location":"web2/#references","text":"https://wiraraja.ac.id/ < https://codeburst.io/web-crawling-and-scraping-in-python-7116b16d27c7 > https://stackoverflow.com/questions/17618981/how-to-sort-pandas-data-frame-using-values-from-several-columns https://networkx.github.io/ https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97 https://www.geeksforgeeks.org/python-pandas-dataframe/ https://stackoverflow.com/questions/50018054/how-to-print-all-rows-of-a-pandas-dataframe-in-the-pycharm-console?noredirect=1&lq=1 https://www.youtube.com/watch?v=uIcime2nBjs https://www.tutorialride.com/data-mining/web-mining.htm https://dzone.com/refcardz/data-mining-discovering-and?chapter=8","title":"References"}]}